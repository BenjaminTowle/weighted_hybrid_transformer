from tokenizer import Tokenizer


class Config:
    def __init__(self, model_name, model_size=100, h=2, num_layers=2, vocab_size=5000, activation="swish",
                 learning_rate=1e-3, decay_freq=1000, decay_rate=0.95, num_steps=100000,
                 validation_samples=50, max_length=20, batch_size=128, num_samples=1000000,
                 momentum=0.95, validation_freq=1000, display_loss_freq=1000,
                 retrieval_candidates=100000, test_samples=100, num_retrieved=100, num_generated=10):
        """
        Config object provides a shared object that specifies all the hyper-parameters used for training, building the
        model and evaluation.
        :param model_name: string name of the model
        :param model_size: dimensionality of the model
        :param h: number of attention heads
        :param num_layers: number of layers
        :param vocab_size: number of unique tokens in the embedding and output softmax layers
        :param activation: activation used in hidden layers
        :param learning_rate: learning rate
        :param decay_freq: number of steps until learning rate is decayed
        :param decay_rate: rate at which learning rate is decayed: learning rate <- learning rate * decay rate
        :param num_steps: number of steps to do training
        :param validation_samples: number of samples to evaluate in validation
        :param max_length: maximum number of tokens per context / response.  Anything longer will be truncated
        :param batch_size: number of samples per batch
        :param num_samples: total number of samples in train dataset
        :param momentum: relative weighting of previous loss versus current loss when displaying loss to the user
        :param validation_freq: number of steps until validation data is evaluated
        :param display_loss_freq: number of steps until loss is displayed to user
        :param retrieval_candidates: number of candidates available for retrieval
        :param test_samples: number of samples used in testing
        :param num_retrieved: number of candidates retrieved by retrieval module
        :param num_generated: number of responses generated by generative module
        """
        assert model_name in ["baseline", "hybrid", "weighted", "weighted_plus", "tfidf"]
        self.model_name = model_name
        self.stopwords = None
        self.stopword_l = None
        self.tokenizer = Tokenizer(lang="en", vs=vocab_size, dim=model_size)
        if model_name == "weighted":
            stopwords = "i no yeah yes what is that the a it sorry and don't know"
            self.stopwords = self.process_stopwords(stopwords)
            self.stopword_l = 0.5

        elif model_name == "weighted_plus":
            stopwords = "i no yeah yes what is that the a it sorry and don't know ? it's ok okay of going to gonna have I you you're not"
            self.stopwords = self.process_stopwords(stopwords)
            self.stopword_l = 0.8

        if model_name == "hybrid":
            self.multitask = True
        else:
            self.multitask = False

        self.model_size = model_size
        self.h = h
        self.num_layers = num_layers
        self.vocab_size = vocab_size
        self.activation = activation
        self.learning_rate = learning_rate
        self.decay_freq = decay_freq
        self.decay_rate = decay_rate
        self.num_steps = num_steps
        self.validation_samples = validation_samples
        self.max_length = max_length
        self.batch_size = batch_size
        self.num_samples = num_samples
        self.momentum = momentum
        self.validation_freq = validation_freq
        self.display_loss_freq = display_loss_freq
        self.retrieval_candidates = retrieval_candidates
        self.test_samples = test_samples
        self.num_retrieved = num_retrieved
        self.num_generated = num_generated

    def process_stopwords(self, stopwords):
        """
        Takes a string of stopwords and processes it into list of ids that the tokenizer can recognise.  Must also
        ensure no duplicates are in list as this will cause divergence.
        :param stopwords: string containing stopwords
        :return: list of stop_ids
        """
        stop_ids = self.tokenizer.encode_ids(stopwords)
        stop_ids.append(2)
        # Remove duplicates
        tmp_stop_ids = []
        for id_ in stop_ids:
            if id_ not in tmp_stop_ids:
                tmp_stop_ids.append(id_)
        stop_ids = tmp_stop_ids

        return stop_ids
        
